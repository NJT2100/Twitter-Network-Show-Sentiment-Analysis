# -*- coding: utf-8 -*-
"""
    This module takes reads in the text content of the raw unprocessed
    tweets which are read in by twitter_stream.py and stored locally. The
    module finds and replaces any unnecessary character strings which are
    not helpful to the categorization/sentiment anaylsis process

    Example:
        $ python preprocessing.py tweets.csv clean_tweets.csv

    Made for: EECS 4415 - Big Data Systems (York University EECS dept.)
    Original Author: Nathan Thomas

"""

import sys
import argparse
import re
import json
import spacy
import pandas as pd
import time
import csv
from google.cloud import translate_v2 as translate

parser = argparse.ArgumentParser(description="Clean the raw Tweets")
parser.add_argument("input_file")
parser.add_argument("output_file")
args = parser.parse_args()

def clean_text(text):
    """
        Uses regular expressions to find unwanted charactes and modify/prune them.

        Args:
            text (string): The tweet to be cleaned
        Returns:
            string: The tweet with unwanted characters removed
    """
    text = re.sub('^b["\'\"]', '', text)            # Remove the opening byte symbol and opeing quotes
    text = re.sub('^RT ', '', text)                 # Remove the retweet symbol and quote from begining of tweet
    text = re.sub('$[\'"]+', '', text)              # Remove the quotes from the end of the tweets
    text = re.sub(r'\\x[\d\w]{2}', '', text)        # Remove unwanted ascii codes which denote unprintable
    text = re.sub(r'\\+u201[89]', "'", text)        # Replace unicode single quotes with ascii single quotes
    text = re.sub(r'\\+u201[cd]', "", text)         # Remove unicode double quotes
    text = re.sub(r'\\+u2026', ".", text)           # Replace elipse with period
    text = re.sub(r'\\+u[\w\d]{4}', '', text)       # Remove all 4 character unicode codes
    text = re.sub(r'\\+U[0]{2}[\w\d]{6}', '', text) # Remove all 8 character unicode codes
    text = re.sub(r'\\+', '', text)                 # Remove all remaining \ from text
    text = re.sub(r'/+', ' ', text)                 # Replace / with ' '
    text = re.sub(r'&gt', '', text)                 # Remove greater than symbols
    text = re.sub(r'[^A-Za-z0-9#@\s]', ' ', text)   # Remove all characters that are not words, numbers, #, @, or whitespace
    text = re.sub(r'[\s]+', ' ', text)              
    text = text.encode().decode('unicode-escape')
    return text.lower()

def clean_tweets(in_file, out_file):
    """
        Reads in each record and removes unwanted characters from both
        the text of the tweet and the location of the tweet.

        Args:
            in_file (string): The name of the file storing the raw data
            out_file (string): The name of the file we will output to
        Returns:
            void: Writes 
    """
    #client = translate.Client()

    # Read in the input file which is the raw tweets generated by twitter_stream.py
    # and clean the text and the language columns
    with open(out_file, mode='w', newline='') as clean_file:
        writer = csv.writer(clean_file)
        writer.writerow(["status_id","in_reply_to_id","user_id","text","language","created_at","location","verified"])
        for data_chunk in pd.read_csv(in_file, chunksize=5000):
            for record in data_chunk.itertuples(index=True, name='Pandas'): 
                text = clean_text(getattr(record, 'text'))
                lang = getattr(record, 'language')
                # connect to google cloud services and translate text
                """ if lang != 'en':
                    translation = client.translate(text)
                    text = translation['translatedText']
                    print(text) """
                columns = [
                    getattr(record, 'status_id'),
                    getattr(record, 'in_reply_to_id'),
                    getattr(record, 'user_id'),
                    text,
                    lang,
                    getattr(record, 'created_at'),
                    clean_text(getattr(record, 'location')),
                    getattr(record, 'verified')]            
                writer.writerow(columns)

def main():
    start = time.time()

    clean_tweets(args.input_file, args.output_file)    
        
    end = time.time()
    print("Finished in %d seconds" % (end-start))
    #appox. 7 minutes per 1GB

if __name__ == "__main__":
    main()